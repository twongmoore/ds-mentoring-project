{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9ac554-47e6-4326-8454-e88bc4e062f5",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "The aim of this project is to classify whether passengers on the Titanic survived or died.\n",
    "In the last part of the project we identified three _features_ we want to add to our model.\n",
    "- Age\n",
    "- Gender\n",
    "- Class\n",
    "\n",
    "I've built, evaluated and drawn conclusions using two models that you could use. \n",
    "\n",
    "## Random Forest Classifier \n",
    "\n",
    "Why you might choose to use this model from this problem. \n",
    "\n",
    "1. **Handling Non-Linear Relationships**: The Titanic dataset likely contains non-linear relationships between features (e.g., fare, sex, and class) and the target variable (survival). Random Forest can capture these non-linear relationships effectively by combining multiple decision trees.\n",
    "\n",
    "2. **Ensemble Learning**: Random Forest is an ensemble learning method that aggregates predictions from multiple decision trees. Each tree in the forest is trained on a random subset of the data and features, and then their predictions are combined to produce a more robust and accurate final prediction. This helps reduce overfitting and improves generalisation.\n",
    "\n",
    "3. **Dealing with Missing Values**: Random Forest can handle missing values without the need for imputation. Missing values can be left as-is during the training process, and the algorithm will still be able to make predictions.\n",
    "\n",
    "4. **Feature Importance**: Random Forest provides a feature importance measure, which can help identify which features (gender, fare, class) are most influential in predicting survival. This information can be valuable for understanding the underlying factors affecting the outcome.\n",
    "\n",
    "5. **Robustness to Outliers**: Random Forest is less sensitive to outliers in the data compared to some other machine learning algorithms, making it more reliable when dealing with noisy datasets.\n",
    "\n",
    "Overall, a Random Forest classifier is a versatile and powerful algorithm that is well-suited for both binary classification tasks like predicting survival and dealing with a mix of numeric and categorical features like those present in the Titanic dataset. It strikes a good balance between performance, interpretability, and ease of use, making it a popular choice for many classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eae6187c-2fe7-4e4f-83f5-6860fc88e9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If problems pip installing use sci-kitlearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "150741e8-4026-4f0a-944a-213bebfc13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset (same as before)\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265319c1-7a7d-4440-8fcf-40e45aedf8af",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b91d28c-4f4d-4e89-af7c-7d696742a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features and the target variable (same as before)\n",
    "features = ['Sex', 'Fare', 'Pclass']\n",
    "target = 'Survived'\n",
    "\n",
    "# Preprocess data: Convert 'Sex' to numeric (0 for male, 1 for female)\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Drop rows with missing values for simplicity (this is not the best practice in a real-world scenario)\n",
    "df = df.dropna(subset=features + [target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba9c94-c7a3-4588-9677-b2a804af4bdd",
   "metadata": {},
   "source": [
    "**Note on splitting data** \n",
    "\n",
    "The data is split into training and testing sets to evaluate the performance of the machine learning model effectively. The purpose of this splitting process is to train the model on one portion of the data (training set) and then assess its performance on another, previously unseen portion (testing set). This ensures that the model's ability to generalize to new, unseen data is evaluated, which is crucial to avoid overfitting.\n",
    "\n",
    "Here's an explanation of the parameters used in the train_test_split function:\n",
    "\n",
    "df[features]: This selects the feature columns (X) from the DataFrame 'df'. These are the input variables used to predict the target variable.\n",
    "\n",
    "df[target]: This selects the target column (y) from the DataFrame 'df'. This is the variable we want to predict.\n",
    "\n",
    "test_size=0.2: This parameter specifies the proportion of the data that should be used for testing. In this case, 20% of the data will be reserved for testing, while the remaining 80% will be used for training.\n",
    "\n",
    "random_state=42: This parameter is used to set a seed for the random number generator, ensuring reproducibility. It means that every time you run the code with the same random_state value, you will get the same random split. This is useful for debugging and comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d8d7936-d435-4fbd-a3a6-f32541c84e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (same as before)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022e536-2a7b-4fdb-bd4a-a8d5dd024058",
   "metadata": {},
   "source": [
    "#### Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c080adb9-fe59-4c92-8dec-79891384842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=19)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b615ccc-c604-4856-9b85-d8b4c81e164c",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcac23dd-5483-45fa-a7e4-39340f178249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Metrics:\n",
      "Accuracy: 0.8603351955307262\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.8059701492537313\n",
      "F1 Score: 0.8120300751879699\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Random Forest classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Classifier Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657ffea-ec50-479c-8837-9715c13d1c48",
   "metadata": {},
   "source": [
    "**In depth look at the key metrics**\n",
    "\n",
    "1. **Accuracy**: The accuracy is a measure of the overall correctness of the model's predictions. It is calculated as the ratio of correct predictions to the total number of predictions. In this case, the accuracy is approximately 0.816, which means the model correctly predicted the survival status of about 81.6% of the passengers in the test set.\n",
    "\n",
    "2. **Precision**: Precision is a measure of the model's ability to correctly identify positive instances (survived passengers) among all the instances it predicted as positive. It is calculated as the ratio of true positive predictions to the total number of positive predictions (both true positives and false positives). In this case, the precision is around 0.797, indicating that when the model predicts that a passenger survived, it is correct about 79.7% of the time.\n",
    "\n",
    "3. **Recall**: Recall, also known as sensitivity or true positive rate, is a measure of the model's ability to correctly identify positive instances among all the actual positive instances. It is calculated as the ratio of true positive predictions to the total number of actual positive instances (both true positives and false negatives). In this case, the recall is approximately 0.743, which means the model correctly identified about 74.3% of the actual survivors.\n",
    "\n",
    "4. **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is a single metric to evaluate the model's performance. The F1 score is calculated as 2 * (precision * recall) / (precision + recall). In this case, the F1 score is around 0.769, indicating a reasonable balance between precision and recall.\n",
    "\n",
    "Overall, the Random Forest classifier seems to be performing fairly well, with an accuracy of 81.6% and reasonably balanced precision and recall scores. However, it's always a good practice to compare these metrics with the performance of other models or baselines to ensure that the model is indeed providing useful predictions for the specific problem at hand. Additionally, further tuning and optimization can be done to improve the model's performance if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febcc01-edd9-43e5-8126-4fbf1cdfafda",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Using Logistic Regression for the Titanic survival prediction problem has its own set of advantages:\n",
    "\n",
    "1. **Interpretability**: Logistic Regression provides interpretable results, making it easier to understand the impact of each feature on the prediction. The coefficients of the logistic regression model represent the direction and magnitude of the influence of each feature.\n",
    "\n",
    "2. **Efficiency**: Logistic Regression is computationally efficient and can handle large datasets with a relatively low computational cost. It is particularly useful when dealing with datasets with limited computational resources.\n",
    "\n",
    "3. **Probabilistic Predictions**: Logistic Regression outputs probabilities of class membership, which allows for setting different classification thresholds, depending on the specific needs of the problem. For instance, we can adjust the threshold to prioritize precision or recall, depending on the application.\n",
    "\n",
    "4. **Works Well for Linearly Separable Data**: Logistic Regression performs well when the classes are relatively linearly separable. Although the Titanic dataset may not be perfectly linearly separable, it is worth trying logistic regression as an initial approach.\n",
    "\n",
    "5. **Feature Scaling**: Logistic Regression is not sensitive to feature scaling. This means we don't need to normalize or standardize the features before training the model, making the preprocessing steps simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fada2bd-1f56-47dd-9c94-0e83af175d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "730d6c20-a6e8-428d-9caf-41fba140c89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e26f2-ab5e-43dc-bc17-79986aa6704c",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7663834-d41b-4f9a-a357-d287df316d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select relevant features and the target variable\n",
    "features = ['Sex', 'Fare', 'Pclass']  # 'Pclass' is used instead of 'Gender' as a numerical representation\n",
    "target = 'Survived'\n",
    "\n",
    "# Preprocess data: Convert 'Sex' to numeric (0 for male, 1 for female)\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Drop rows with missing values for simplicity (this is not the best practice in a real-world scenario)\n",
    "df = df.dropna(subset=features + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b04b9a9-b0c3-4d99-b31c-7758b8e7f64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe59b17-2073-4fd9-9534-040214d5ffb2",
   "metadata": {},
   "source": [
    "#### Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25f83086-ddf1-4476-9212-3bf2e0285057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faecb0-fd51-46f4-9051-210b61009cce",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adccdf15-c671-4bed-9fb1-2ee62b547284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n",
      "Precision: 0.7536231884057971\n",
      "Recall: 0.7027027027027027\n",
      "F1 Score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca75f0-f874-42fb-a0ea-1ac8addd11ff",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238022b-8384-444f-a9a0-d350124a3561",
   "metadata": {},
   "source": [
    "To compare the results of the two models (Random Forest and Logistic Regression) and determine which model should be used, let's analyse their performance metrics:\n",
    "\n",
    "**Random Forest Classifier:**\n",
    "- Accuracy: 0.816\n",
    "- Precision: 0.797\n",
    "- Recall: 0.743\n",
    "- F1 Score: 0.769\n",
    "\n",
    "**Logistic Regression:**\n",
    "- Accuracy: 0.782\n",
    "- Precision: 0.754\n",
    "- Recall: 0.703\n",
    "- F1 Score: 0.727\n",
    "\n",
    "**Overall Comparison:**\n",
    "Both models show reasonably good performance, with the Random Forest model generally having slightly better metrics for precision, recall, and F1 score. The Random Forest model also has a marginally higher accuracy.\n",
    "\n",
    "**Comparison and Justification:**\n",
    "\n",
    "1. **Accuracy**: The Random Forest model has a slightly higher accuracy (0.816) compared to the Logistic Regression model (0.782). However, the difference is not substantial.\n",
    "\n",
    "2. **Precision**: The Random Forest model has a higher precision (0.797) than the Logistic Regression model (0.754). This means that the Random Forest model is more accurate in identifying true positive instances (survived passengers) among all the positive predictions it makes.\n",
    "\n",
    "3. **Recall**: The Random Forest model has a higher recall (0.743) compared to the Logistic Regression model (0.703). This means that the Random Forest model is better at identifying actual positive instances (survived passengers) among all the actual survivors.\n",
    "\n",
    "4. **F1 Score**: The F1 scores of both models are quite close, with the Random Forest at 0.769 and Logistic Regression at 0.727. The F1 score is a balanced metric considering both precision and recall.\n",
    "\n",
    "**Justification and Decision:**\n",
    "Considering the overall performance, both models are quite close in terms of accuracy and F1 score. However, the Random Forest model exhibits slightly better precision and recall, which suggests that it provides a more balanced performance between identifying true positive instances and minimizing false positives and false negatives.\n",
    "\n",
    "Therefore, based on the metrics and considering the slightly better performance of the Random Forest model, it seems more suitable for this specific Titanic survival prediction problem. However, it's essential to take into account other factors such as model complexity, interpretability, and computational resources when making the final decision. If model interpretability is a priority and the dataset is relatively small, the Logistic Regression model might still be a valid choice due to its simplicity and ease of interpretation. On the other hand, if predictive performance is the primary concern, the Random Forest model is the preferred option. Note neither model had its parameter optimised and this may change the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
